{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85czpN5uqmpN"
      },
      "source": [
        "# Minimum Viable Product - ATS system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LqOKhvHpkfH"
      },
      "source": [
        "**Instructions:** Create your own Groq and Gemini API key. Add either of them, or both to the secrets in colab notebook as GROQ_API_KEY and GEMINI_API_KEY respectfully. Then run all cells from Runtime -> Run all.\n",
        "\n",
        "LAUNCH_MODE=True is an option to generate fronted for analysis requird. No frontend is generated in opposite scenario and the debug sections are primarily written for purpose of packaging the system in future."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6CDgwAcOIjc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjFPpIUSpTGX"
      },
      "outputs": [],
      "source": [
        "# Install necessary requirements\n",
        "!pip install -q google-generativeai groq gradio PyPDF2 torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7w7R-iQr1jL"
      },
      "outputs": [],
      "source": [
        "# Make valid imports\n",
        "import google.generativeai as genai\n",
        "import gradio as gr\n",
        "import os\n",
        "import PyPDF2\n",
        "import re\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from google.colab import auth, userdata\n",
        "from groq import Groq\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# If False, it defaults to debugging\n",
        "LAUNCH_MODE = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kp0ZCyw-2WD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Jv2PHCFbyUPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKJeOlz1toTF"
      },
      "outputs": [],
      "source": [
        "# Function to read API key file: Supports local development of code\n",
        "def get_api_key(key_path):\n",
        "  try:\n",
        "    with open(file_path, 'r') as file:\n",
        "      return file.read().strip()\n",
        "  except FileNotFoundError:\n",
        "    raise ValueError(f\"API key file not found in pre-determined location: {key_path}. Please provide a valid file\")\n",
        "\n",
        "# Support\n",
        "technologies = [\"Gemini\", \"Groq\"]\n",
        "\n",
        "# Fetch API keys and create communicators\n",
        "if LAUNCH_MODE:\n",
        "  auth.authenticate_user()\n",
        "  api_keys = {tech: userdata.get(f\"{tech.upper()}_API_KEY\") for tech in technologies}\n",
        "else:\n",
        "  key_paths = {\n",
        "    \"Gemini\": \".secrets/gemini_api_key.txt\",\n",
        "    \"Groq\": \".secrets/groq_api_key.txt\"\n",
        "  }\n",
        "  api_keys = {tech: get_api_key(key_paths[tech]) for tech in technologies}\n",
        "\n",
        "# Identify if any keys are missing\n",
        "missing_techs = [tech for tech, key in api_keys.items() if not key]\n",
        "for tech in missing_techs:\n",
        "  technologies.remove(tech)\n",
        "  print(f\"Warning: {tech.capitalize()} API key not found. Removing {tech} from available technologies.\")\n",
        "\n",
        "if not technologies:\n",
        "  if LAUNCH_MODE:\n",
        "    raise SystemExit(\"Cannot initiate the instance as no API keys are found. Set them using Secrets in colab notebook or userdata.save('...')\")\n",
        "  else:\n",
        "    raise SystemExit(\"Cannot initiate the instance as no API keys are found. Verify if the files are properly structured.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2kj77tbxNns"
      },
      "outputs": [],
      "source": [
        "\"\"\" Text processor functions \"\"\"\n",
        "def read_from_pdf(pdf_file_path):\n",
        "    with open(pdf_file_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = ''.join(page.extract_text() for page in pdf_reader.pages)\n",
        "    return text\n",
        "\n",
        "def read_from_text(text_file_path):\n",
        "    with open(text_file_path, 'r', encoding='utf_8') as text_file:\n",
        "        return text_file.read()\n",
        "\n",
        "def text_to_comma_seperated(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\*\", \"\", text)\n",
        "    # Remove subheadings (Skills, Experience, Education)\n",
        "    text = re.sub(r\"(skills|experience|education):\", \"\", text)\n",
        "    text = re.sub(r\"[.,:]\", \",\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    text = re.sub(r\",\\s*\", \",\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\"\"\" Text extractor functions \"\"\"\n",
        "def genai_text_extractor(text, tech):\n",
        "    if tech == \"Gemini\":\n",
        "        prompt = f\"\"\"Act as a efficient ATS system. Summarize the text into data suitable for input to a RoBERTa model and in following format\n",
        "        Skills: List skills here (Technical and Soft-skills), \\n\n",
        "        Experience: List number of years of experience here. Mention only 1 most relevant title. \\n\n",
        "        Education: List degrees here. Mention highest degree if candidate has Dual degree. Convert abbreviations to only Bachelors, Masters etc. Discard abbreviations in generation.\n",
        "        Ensure the result is concise. Text: {text}\"\"\"\n",
        "        genai.configure(api_key=userdata.get(f\"{tech.upper()}_API_KEY\"))\n",
        "        # Selecting a gemini model depending on the plan (Free in this instance)\n",
        "        model = genai.GenerativeModel(\"gemini-pro\")\n",
        "        response = model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=genai.types.GenerationConfig(\n",
        "                candidate_count = 1,\n",
        "                max_output_tokens = 512,\n",
        "                temperature = 0.1\n",
        "            )\n",
        "        )\n",
        "        return response.text\n",
        "\n",
        "    if tech == \"Groq\":\n",
        "        prompt = f\"\"\"Act as an efficient ATS system. Directly summarize the provided text into data suitable for input to a RoBERTa model.\n",
        "            Output only in the exact format specified below, with no introductory phrases, explanations, or additional context.\n",
        "            Skills: List all relevant skills here (technical and soft skills).\n",
        "            Experience: Provide the number of years of experience and mention only the most relevant title.\n",
        "            Education: Provide the highest degree achieved. If the candidate has a dual degree, mention only the highest.\n",
        "            Do not include abbreviations like B.Tech or M.Techâ€”use 'Bachelors,' 'Masters,' etc., instead.\n",
        "            Ensure the response is concise and strictly follows the specified format. Text: {text}\"\"\"\n",
        "        client = Groq(api_key=userdata.get(f\"{tech.upper()}_API_KEY\"))\n",
        "        response = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"user\",\n",
        "                \"content\": prompt}],\n",
        "            # Selecting a Llama model depending on the plan (Free in this instance)\n",
        "            model=\"llama3-70b-8192\"\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\"\"\" Text similarity functions \"\"\"\n",
        "def calculate_resume_similarity(resume_text, job_description_text):\n",
        "    \"\"\"Calculates similarity score between resume and job description.\"\"\"\n",
        "    model_name = \"cross-encoder/stsb-roberta-base\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "    inputs = tokenizer(resume_text, job_description_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        similarity_score = torch.sigmoid(outputs.logits).item()\n",
        "    return similarity_score\n",
        "\n",
        "# --- Fit Categorization ---\n",
        "def categorize_fit(similarity_score):\n",
        "    \"\"\"Categorizes fit based on similarity score.\"\"\"\n",
        "    fit_percentage = similarity_score * 100\n",
        "    if fit_percentage >= 75:\n",
        "        return \"Good Fit\", fit_percentage\n",
        "    elif fit_percentage >= 50:\n",
        "        return \"Moderate Fit\", fit_percentage\n",
        "    else:\n",
        "        return \"Not a Good Fit\", fit_percentage\n",
        "\n",
        "# --- Communication Generation ---\n",
        "def communication_generator(message, matching_skills, fit_category):\n",
        "    \"\"\"Generates a communication response based on the input message and fit category.\"\"\"\n",
        "    return (f\"{message} The matching skills are: {', '.join(matching_skills)}. \"\n",
        "            f\"This candidate is considered a {fit_category}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAXtAeg10mM2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Analyzes the resume and job description.\"\"\"\n",
        "def analyse_document(resume_path, job_description_path, tech):\n",
        "\n",
        "    if tech is None:\n",
        "        return \"Error: Select a technology\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "    # Extract resume text based on the file type\n",
        "    if os.path.splitext(resume_path)[1] == '.pdf':\n",
        "        resume_text = read_from_pdf(resume_path)\n",
        "    elif os.path.splitext(resume_path)[1] == '.txt':\n",
        "        resume_text = read_from_text(resume_path)\n",
        "    else:\n",
        "        return \"Invalid file type. Please upload a PDF or TXT file for the resume.\"\n",
        "\n",
        "    # Extract job description text\n",
        "    if os.path.splitext(job_description_path)[1] == '.txt':\n",
        "        job_description_text = read_from_text(job_description_path)\n",
        "    else:\n",
        "        return \"Invalid file type. Please upload a TXT file for the job description.\"\n",
        "\n",
        "    analysed_resume = genai_text_extractor(resume_text, tech)\n",
        "    analysed_job_description = genai_text_extractor(job_description_text, tech)\n",
        "\n",
        "    # Identify matching skills\n",
        "    resume_skill_set = text_to_comma_seperated(analysed_resume).split(',')\n",
        "    job_description_skill_set = text_to_comma_seperated(analysed_job_description).split(',')\n",
        "    matching_skills = list(set(resume_skill_set) & set(job_description_skill_set))\n",
        "\n",
        "    # Calculate similarity score USING PROCESSED RESUME TEXT AND JOB DESCRIPTION TEXT THAN JUST PDF EXTRACTED TEXT\n",
        "    similarity_score = calculate_resume_similarity(analysed_resume, analysed_job_description)\n",
        "    fit_category, fit_percentage = categorize_fit(similarity_score)\n",
        "\n",
        "    # Generate communication response\n",
        "    communication_response = communication_generator(\n",
        "        f\"The candidate has the following skills: {', '.join(resume_skill_set)}.\",\n",
        "        matching_skills,\n",
        "        fit_category\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        \"Success\",\n",
        "        f\"Similarity score: {similarity_score*100:.2f}%\",\n",
        "        communication_response,\n",
        "        \", \".join(resume_skill_set),\n",
        "        \", \".join(job_description_skill_set),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18BuvDan05cJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"Frontend manager for the ATS system\"\"\"\n",
        "def __set_technology(tech):\n",
        "    if tech:\n",
        "        return gr.update(value=f\"Running analysis using technology: {tech.upper()}\", visible=True)\n",
        "    return gr.update(visible=False)\n",
        "\n",
        "def interface(technologies):\n",
        "    with gr.Blocks() as demo:\n",
        "        selected_tech = gr.State(None)  # To store selected technology\n",
        "        with gr.Tab(\"Resume Analysis\"):\n",
        "          # Technology selection dropdown\n",
        "          gr.Markdown(\"### Technology Selection\")\n",
        "          tech_dropdown = gr.Dropdown(\n",
        "              label=\"Choose a model from the available API keys\",\n",
        "              choices=technologies,\n",
        "              interactive=True,\n",
        "              value=None,\n",
        "          )\n",
        "          selected_tech_display = gr.Markdown(visible=False)\n",
        "          tech_dropdown.change(fn=__set_technology, inputs=tech_dropdown, outputs=selected_tech_display)\n",
        "\n",
        "          with gr.Row():\n",
        "              # Inputs on the left\n",
        "              with gr.Column(scale=1):  # Adjust scale for layout proportions\n",
        "                  gr.Markdown(\"### Upload Files\")\n",
        "                  resume_input = gr.File(label=\"Upload Resume (.PDF or .TXT)\")\n",
        "                  job_input = gr.File(label=\"Upload Job Description (.TXT)\")\n",
        "                  tech_dropdown\n",
        "                  analyse_button = gr.Button(\"Submit\")\n",
        "\n",
        "              # Outputs on the right\n",
        "              with gr.Column(scale=2):  # Adjust scale for layout proportions\n",
        "                  gr.Markdown(\"### Results\")\n",
        "                  status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "                  similarity_score = gr.Textbox(label=\"Similarity Score\", interactive=False)\n",
        "                  tool_response = gr.Textbox(label=\"Tool Response\", interactive=False)\n",
        "                  resume_skills = gr.Textbox(label=\"Identified Skills in Resume\", interactive=False)\n",
        "                  job_skills = gr.Textbox(label=\"Defined Skills in Job Description\", interactive=False)\n",
        "\n",
        "          analyse_button.click(\n",
        "              fn=analyse_document,\n",
        "              inputs=[resume_input, job_input, tech_dropdown],\n",
        "              outputs=[status, similarity_score, tool_response, resume_skills, job_skills],\n",
        "          )\n",
        "\n",
        "    return demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3XzfJ6Sd1LjP",
        "outputId": "47e2e66e-5b04-4a7d-c030-38d738991159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7ea26f2f967bde49e3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7ea26f2f967bde49e3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 404.66ms\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2108, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1655, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-5-abe60c62dd2c>\", line 21, in analyse_document\n",
            "    analysed_resume = genai_text_extractor(resume_text, tech)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-4-e366c946b619>\", line 34, in genai_text_extractor\n",
            "    response = model.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1161, in __call__\n",
            "    raise core_exceptions.from_http_response(response)\n",
            "google.api_core.exceptions.NotFound: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://7ea26f2f967bde49e3.gradio.live\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Main script to combine everything\"\"\"\n",
        "\n",
        "if LAUNCH_MODE:\n",
        "  demo = interface(technologies)\n",
        "  demo.launch(debug=True, share=True)\n",
        "else:\n",
        "  resume_path = \"documents/resume.pdf\"\n",
        "  job_description_path = \"documents/job_description.txt\"\n",
        "  for tech in technologies:\n",
        "    print(f\"\\nRunning analysis using technology: {tech.capitalize()}\")\n",
        "    _, similarity_score, communication_response, _, _ = analyse_document(resume_path, job_description_path, tech)\n",
        "    print(f\"Similarity score: {similarity_score}\")\n",
        "    print(communication_response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JyQT6rB82Z62"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}